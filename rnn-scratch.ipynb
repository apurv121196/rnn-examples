{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_loss(vocab_size, seq_length):\n",
    "    return -np.log(1.0/vocab_size)*seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(loss, cur_loss):\n",
    "    return loss * 0.999 + cur_loss * 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:]  # capitalize first character \n",
    "    print ('%s' % (txt, ), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "#     np.random.seed(1)\n",
    "    Wax = np.random.randn(n_a, n_x)*0.01 \n",
    "    Waa = np.random.randn(n_a, n_a)*0.01 \n",
    "    Wya = np.random.randn(n_y, n_a)*0.01 \n",
    "    ba = np.zeros((n_a, 1)) \n",
    "    by = np.zeros((n_y, 1)) \n",
    "    \n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba,\"by\": by}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, lr):\n",
    "\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['ba']  += -lr * gradients['dba']\n",
    "    parameters['by']  += -lr * gradients['dby']\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(parameters, a_prev, x):\n",
    "    \n",
    "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + ba) \n",
    "    p_t = softmax(np.dot(Wya, a_next) + by)  \n",
    "    \n",
    "    return a_next, p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(X, Y, a0, parameters, vocab_size = 27):\n",
    "    \n",
    "    x, a, y_hat = {}, {}, {}\n",
    "    \n",
    "    a[-1] = np.copy(a0)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(X)):\n",
    "        \n",
    "        x[t] = np.zeros((vocab_size,1)) \n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "        \n",
    "        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], x[t])\n",
    "        \n",
    "        loss -= np.log(y_hat[t][Y[t],0])\n",
    "        \n",
    "    cache = (y_hat, a, x)\n",
    "        \n",
    "    return loss, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    \n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] \n",
    "    dz = (1 - a * a) * da \n",
    "    gradients['dba'] += dz\n",
    "    gradients['dWax'] += np.dot(dz, x.T)\n",
    "    gradients['dWaa'] += np.dot(dz, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, dz)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    gradients = {}\n",
    "    \n",
    "    (y_hat, a, x) = cache\n",
    "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "    \n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['dba'], gradients['dby'] = np.zeros_like(ba), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "    \n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n",
    "    \n",
    "    return gradients, a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='names.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "There are 45594 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open(file_name, 'r').read()\n",
    "data= data.lower()\n",
    "print(type(data))\n",
    "# print(data.shape, data)\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w', '\\n', 'l', 'b', 'g', 'e', 'r', 'u', 'z', 'o', 's', 'x', 'f', 'k', 'm', 'a', 'p', 'd', 'h', 'c', 'n', 'y', 'q', 't', 'j', 'v', 'i']\n",
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'} {'w': 23, 'a': 1, '\\n': 0, 'l': 12, 'b': 2, 'p': 16, 'g': 7, 'c': 3, 'h': 8, 'e': 5, 'd': 4, 'r': 18, 'n': 14, 'y': 25, 'u': 21, 'q': 17, 't': 20, 'z': 26, 'j': 10, 'o': 15, 'v': 22, 's': 19, 'x': 24, 'f': 6, 'k': 11, 'i': 9, 'm': 13}\n"
     ]
    }
   ],
   "source": [
    "print(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char, char_to_ix)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    \n",
    "    dWaa, dWax, dWya, dba, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['dba'], gradients['dby']\n",
    "   \n",
    "    for gradient in [dWax, dWaa, dWya, dba, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out = gradient)\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"dba\": dba, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample(parameters, char_to_ix, seed):\n",
    "def sample(parameters, char_to_ix):\n",
    "    \n",
    "    \n",
    "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    indices = []\n",
    "    \n",
    "    idx = -1 \n",
    "    \n",
    "    \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + ba)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "        \n",
    "#         np.random.seed(counter+seed) \n",
    "        \n",
    "        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n",
    "\n",
    "        indices.append(idx)\n",
    "        \n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        a_prev = a\n",
    "        \n",
    "#         seed += 1\n",
    "        counter +=1\n",
    "        \n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \n",
    "    \n",
    "    \n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    gradients = clip(gradients, 5)\n",
    "    \n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n",
    "    \n",
    "    \n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    with open(file_name) as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "#     np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]]\n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "        \n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n",
    "        \n",
    "        \n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "#             seed = 0\n",
    "            for name in range(dino_names):\n",
    "                \n",
    "#                 sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                sampled_indices = sample(parameters, char_to_ix)\n",
    "                print_sample(sampled_indices, ix_to_char)\n",
    "                \n",
    "#                 seed += 1  \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 19.775981\n",
      "\n",
      "Rkwixnxtesobjgnnerlpypkpuvilxjpttccivdqfqjvvgcabbc\n",
      "Kceyvbelazuhxjtnzoosktayancrhpzixrscbnj\n",
      "Nsylo\n",
      "Fjmxgegsoga\n",
      "Gtexqtubftxawbxeozodwkdco\n",
      "Rmgex\n",
      "Baxhxmjgpjtwivjveayek\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 20.178975\n",
      "\n",
      "Anq\n",
      "Aj\n",
      "Ot\n",
      "Jam\n",
      "\n",
      "Ibikikm\n",
      "\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 17.995098\n",
      "\n",
      "Bhurazin\n",
      "Sirit\n",
      "Eep\n",
      "Amavcherbaminan\n",
      "Aninjardoh\n",
      "Dik\n",
      "Sunveed\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 14.057537\n",
      "\n",
      "Mramisogudashujilajamajajurantapes\n",
      "Inu\n",
      "Gojnimhenhaghushish\n",
      "Jabandrajulakh\n",
      "Rakenday\n",
      "Malesh\n",
      "Aghak\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 19.967727\n",
      "\n",
      "Ragashun\n",
      "Aokam\n",
      "Sinay\n",
      "Jinden\n",
      "Sohaan\n",
      "Munad\n",
      "Aepam\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 12.093945\n",
      "\n",
      "Fuinakh\n",
      "Mannder\n",
      "Manur\n",
      "Muaneek\n",
      "Eppandendrravit\n",
      "Muthan\n",
      "Amesh\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 16.956051\n",
      "\n",
      "Sulunk\n",
      "Sont\n",
      "Mundeer\n",
      "Ak\n",
      "Satan\n",
      "Vukesh\n",
      "Deepa\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 12.176500\n",
      "\n",
      "Mintarsjesh\n",
      "Birujal\n",
      "Moram\n",
      "Wayfunsh\n",
      "Kunish\n",
      "Ovtukash\n",
      "Antouk\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 14.263094\n",
      "\n",
      "Andeet\n",
      "Rangu\n",
      "Mohir\n",
      "Mamman\n",
      "Ovu\n",
      "Kudeep\n",
      "Deepal\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 9.374336\n",
      "\n",
      "Santhntul\n",
      "Sandeep\n",
      "Onit\n",
      "Vadet\n",
      "Oripra\n",
      "Kip\n",
      "Deepak\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 14.380102\n",
      "\n",
      "Kichu\n",
      "Rahul\n",
      "Joyan\n",
      "Bbradhanlvir\n",
      "Desh\n",
      "Al\n",
      "Gugeer\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 16.476669\n",
      "\n",
      "Arish\n",
      "Kymkij\n",
      "Shmradesh\n",
      "Alesh\n",
      "Aumak\n",
      "Amikukay\n",
      "Vikkyudvi\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 20.691159\n",
      "\n",
      "Jayenm\n",
      "Pitender\n",
      "Doopanlas\n",
      "Rajeen\n",
      "Mosit\n",
      "Ukhim\n",
      "Aretd\n",
      "\n",
      "\n",
      "Iteration: 26000, Loss: 8.651886\n",
      "\n",
      "Minwal\n",
      "Sastan\n",
      "Deepak\n",
      "Maraj\n",
      "Deepak\n",
      "Nairav\n",
      "Arzay\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 15.101811\n",
      "\n",
      "Naknilindram\n",
      "Shod\n",
      "Kamshnjan\n",
      "Apan\n",
      "Deenna\n",
      "Antu\n",
      "Asul\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 13.903149\n",
      "\n",
      "Deepakush\n",
      "Huler\n",
      "Raxsha\n",
      "Khander\n",
      "Mandeer\n",
      "Mohim\n",
      "Ravesh\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 24.703343\n",
      "\n",
      "Vinad\n",
      "Amsar\n",
      "Asman\n",
      "Vijesh\n",
      "Arka\n",
      "Jital\n",
      "Jitesh\n",
      "\n",
      "\n",
      "Iteration: 34000, Loss: 16.786389\n",
      "\n",
      "Manit\n",
      "Aburad\n",
      "Bhirar\n",
      "Sanjupramend\n",
      "Hipar\n",
      "Arjishen\n",
      "Ajich\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(data)\n",
    "parameters = model(data, ix_to_char, char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
